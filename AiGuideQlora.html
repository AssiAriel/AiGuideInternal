<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Guide - QloRA</title>
    <link rel="icon" href="favicon.png" type="image/png">
    <link rel="stylesheet" href="style.css"> 
</head>

<!----------------------------------------------------------------------------------------------->

<body>
    <p class="TitleHead">AI Guide - QLoRA</p>  

    <!--- Blueprint -->
    <p class="TitleList1Blue">Blueprint</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- General QLoRA Blueprint -->
		<p class="TitleList2Blue">General QLoRA Blueprint</p>
     	<ol>
            <!--- L2 -->
 			<li>List
            <ol>
                <!--- L3 -->
                <li>Workstation: i7-14700 64GB + RTX 5070 Ti 16GB + 990PRO NVMe 2TB
				</li>

                <!--- L3 -->
                <li>Model: Llama 3.1 (8B)
				</li>

                <!--- L3 -->
                <li>Dataset: Alpaca dataset
				</li>

                <!--- L3 -->
                <li>Engine: Unsloth
				</li>

                <!--- L3 -->
                <li>OS: WSL2 (Ubuntu)
				</li>

                <!--- L3 -->
                <li>Code and environment file system: Linux ~/dev/AiLab
				</li>

                <!--- L3 -->
                <li>Models file system: Windows DevDrive D:\ModelManagers\HuggingFace\Models\
				</li>

                <!--- L3 -->
                <li>Also using :Hugging Face, bitsandbytes, triton, torch, transformers & peft
				</li>
			</ol>
 		    </li>
		</ol>
		</li>
    </ol>
 
 <!----------------------------------------------------------------------------------------------->

    <!--- Linux -->
    <p class="TitleList1Blue">Linux WSL2</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Install WSL -->
		<p class="TitleList2Blue">Install WSL</p>
     	<ol>
            <!--- L2 -->
   			<li>Install
 	     	<p>Open Terminal as Administrator
            <br>type: wsl --install
            <br>when it is done restart your computer
            <br>After restart, a black window will pop up asking for a Username and Password,
            <br>(choose simple ones like: dev, dev). 
            <br>If no window pops up, open Terminal and type: wsl
            <br>if it start installing then good,
            <br>if it says "...no installed distributions" then
            <br>use 'wsl.exe --list --online' to list available distributions
            <br>type: wsl --install -d Ubuntu-24.04
            <br>If it looks like it stopped on something like "Create a default Unix user account: ssi"
            <br>then actually, it is just a print over print.
            <br>Backspace 3 times to remove the "ssi" string
            <br>and enter your user name (dev)
            <br>and then password.</p>
 		    </li>

            <!--- L2 -->
   			<li>Uninstall
 	     	<p>If you need to uninstall, open Terminal
            <br>type: wsl --unregister Ubuntu-24.04
            <br>type: wsl --install -d Ubuntu-24.04</p>
 		    </li>
            
            <!--- L2 -->
            <li>Check
            <ol>
                <!--- L3 -->
                <li>Check that you got your user prompt on your computer
				</li>

                <!--- L3 -->
                <li>Inside Ubuntu type: nvidia-smi
                <br>you should see the GPU card there
           	    <p><img src="images/FirstLinuxRun.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
                </li>
                
                <!--- L3 -->
                <li>On windows Terminal, you can check that it is the right version 
           	    <p><img src="images/LinuxVer.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
				</li>
			</ol>
 		    </li>

            <!--- L2 -->
            <li>Symbolic Link
            <ol>
                <!--- L3 -->
                <li>Create a folder in WIndows D:\ModelManagers\HuggingFace\Models
				</li>

                <!--- L3 -->
                <li>Move from the Windows user folder to your Linux home folder
                <br>type: cd ~
                <br>type: ln -s /mnt/d/ModelManagers/HuggingFace/Models ~/Models
                <br>To check type: cd ~/Models
                <br>type: pwd
           	    <p><img src="images/LinuxSymbolicLink.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
				</li>
			</ol>
 		    </li>

            <!--- L2 -->
   			<li>Update Linux
 	     	<p>Type: sudo apt update && sudo apt upgrade -y</p>
 		    </li>

            <!--- L2 -->
   			<li>Opening Ubuntu window
 	     	<p>Open Terminal and type: wsl
            <br>or, open terminal and click the little dropdown arrow at the top and select Ubuntu 24.04
            <br>or, right click the Desktop, 
            <br>New > Shortcut
            <br>location type: wsl.exe ~ -d Ubuntu-24.04
            <br>name: "AiLab Workstation"</p>
 		    </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- Miniconda and Python -->
    <p class="TitleList1Blue">Miniconda and Python</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Install Miniconda -->
		<p class="TitleList2Blue">Install Miniconda</p>
     	<ol>
            <!--- L2 -->
   			<li>Download Installation
 	     	<p>Inside Ubuntu type: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</p>
 		    </li>

            <!--- L2 -->
   			<li>Install
            <p>Type: bash Miniconda3-latest-Linux-x86_64.sh
            <br>you get few "Yes" and "Enter"</p>
 		    </li>

            <!--- L2 -->
   			<li>Activate
            <p>Type: source ~/.bashrc
            <br>and you get "(base)" in the beginning of your prompt</p>
 		    </li>

            <!--- L2 -->
   			<li>Create a room
            <p>Type: conda create --name ailab python=3.11 -y
            <br>it might not run but give you a message that you need to accept terms,
            <br>so you need to run the two lines it gives you there.
            <br>This command creates a "room", an environment, called ailab
            <br>where specific versions of installation (like Python) reside,
            <br>note that this is not the latest Python version - but the latest stable one for Llama3.1</p>
 		    </li>

            <!--- L2 -->
   			<li>Enter the room
            <p>Type: conda activate ailab
            <br>and you get "(ailab)" in the beginning of your prompt</p>
 		    </li>

            <!--- L2 -->
            <li>Clean the  installation file
            <p>Type: rm Miniconda3-latest-Linux-x86_64.sh</p>
 		    </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- PyTorch -->
    <p class="TitleList1Blue">PyTorch</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Install PyTorch -->
		<p class="TitleList2Blue">Install PyTorch</p>
     	<ol>
            <!--- L2 -->
   			<li>Install PyTorch
            <p>From inside your ailab room "(ailab) dev@AssiWorkstation:~$"
            <br>type: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
            <br>you get many downloads and many colors...</p>
 		    </li>
            
            <!--- L2 -->
   			<li>Verify that Python talks to the GPU card
            <p>type: python
            <br>type: import torch
            <br>type: print(f"CUDA available: {torch.cuda.is_available()}")
            <br>type: print(f"GPU Name: {torch.cuda.get_device_name(0)}")
            <br>type: exit()
            <br>and actually, it didn't work for me, this version doesn't support sm_120 which is the CUDA architecture of my card:
           	<p><img src="images/LinuxPytorchWarning.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
            <p>So I need to install a newer version
            <br>from inside ailab room "(ailab) dev@AssiWorkstation:~$"
            <br>type: pip uninstall torch torchvision torchaudio -y
            <br>type: pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
            <br>now it is better:
            <p><img src="images/LinuxPytorchInstall.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
 		    </li>

            <!--- L2 -->
   			<li>And verify some more
            <p>Verify that (for my specific card) sm_120 is part of python
            <br>verify that  Persistence-M is On
           	<p><img src="images/LinuxCheckSmi.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
 		    </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- RAM -->
    <p class="TitleList1Blue">RAM</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Allocate RAM -->
		<p class="TitleList2Blue">Allocate RAM</p>
     	<ol>
            <!--- L2 -->
  			<li>Check RAM
            <p>Check how much RAM is allocated for Linux
            <br>type: free -h
            <p><img src="images/LinuxRam1.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
            <p>This is not so good, I have 64GB of RAM and only 32GB is allocated for Linux (50% is the default), 
            <br>I want to have more RAM for the Linux model training,
            <br>(and actually I don't really think it needs it for Llama 3.1 QLoRA, but...),
            <br>on the other hand I want to leave enough RAM for Windows to run WSL, for we to work parallel, and just to keep things calm...</p>
   		    </li>

            <!--- L2 -->
   			<li>Allocate RAM
            <ol>
                <!--- L3 -->
                <li>On windows, open Notepad,
                <br>type this in plain text, only this:
                <br>[wsl2]
                <br>memory=48GB
                <br>Save the file with this name in your user place C:\Users\Assi\.wslconfig
				</li>

                <!--- L3 -->
                <li>In the Ubuntu window,
                <br> type:exit
				</li>

                <!--- L3 -->
                <li>In the Windows Terminal
                <br>type: wsl --shutdown
				</li>

                <!--- L3 -->
                <li>Open Ubuntu window again
                <br>check now
				</li>
            </ol>
            </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- Unsloth -->
    <p class="TitleList1Blue">Unsloth</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Install Unsloth -->
		<p class="TitleList2Blue">Install Unsloth</p>
     	<ol>
            <!--- L2 -->
   			<li>Install Unsloth
            <ol>
                <!--- L3 -->
                <li>Use a "No Dependencies" installation,
                <br>so that Unsloth wouldn't overwrite the Python version that we just installed with other "recommended" version.
                <br>Type: "conda activate ailab" to enter ailab room
				</li>

                <!--- L3 -->
                <li>Type: pip install --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
                <br>This installs Unsloth
				</li>

                <!--- L3 -->
                <li>Type: pip install --no-deps trl peft accelerate bitsandbytes
                <br>This installs helper libraries
				</li>

                <!--- L3 -->
                <li>Type: pip install --upgrade transformers
                <br>Update the Transformers,
                <br>and we get error
              	<p><img src="images/UnslothInstallError1.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
                <p>We get this error because we installed with no dependencies, becaus we wanted to keep our specific version of Python,
                <br>because we needed a newer version of Python to support our new in market GPU,
                <br>probably in time all these will not be needed as the new GPU wil be supported in the common packages.</p>
				</li>

                <!--- L3 -->
                <li>Type: pip install --no-deps datasets==3.0.0
				</li>

                <!--- L3 -->
                <li>Type: pip install psutil
				</li>
                
                <!--- L3 -->
                <li>When install passes run a test script:
                <br>copy this code to a file:
                <br><a href="texts/check_blackwell.py" download>Download "check_blackwell.py" file</a>
                <br>save the file as check_blackwell.py
                <br>copy the file to ailab in Linux,
                <br>one simple way is to type in ailab: "explorer.exe ." and a Windows folder opens for you to copy to,
                <br>run it there.
              	<p><img src="images/CheckUnsloth.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
                <p>In my case it found some missing or dependencies errors, so I installed the package below,
                <br>if you get errors just follow the messages you get.</p>
               </li>

                <!--- L3 -->
                <li>Type: pip install --no-deps unsloth_zoo
				</li>
            </ol>
            </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- HuggingFace -->
    <p class="TitleList1Blue">HuggingFace</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- HuggingFace Token -->
		<p class="TitleList2Blue">HuggingFace Token</p>
     	<ol>
            <!--- L2 -->
   			<li>Create Token
            <ol>
                <!--- L3 -->
                <li>For Llama-3.1-8B on HuggingFace go to:
                <p><a href="https://huggingface.co/meta-llama/Llama-3.1-8B" target="_blank">HuggingFace - Llama-3.1-8B</a></p>
                </li>

                <!--- L3 -->
                <li>Sign in
                <p>(or create an account)</p>
                </li>

                <!--- L3 -->
                <li>Access Request
                <p>Fill the Access Request form to get access to Llama-3.1-8B
                <br>wait for it to be approved (it takes several minutes - you wil get a mail)</p>
                </li>

                <!--- L3 -->
                <li>Create Token
                <p>Go to Settings -> Access Tokens,
                <br>go to "Read"Tab
                <br>Give it a name (AssiWorkstationHFToken)
                <br>"Create token" and copy it (you must copy it now)</p>
   		        </li>
            </ol>
            </li>

            <!--- L2 -->
   			<li>Install HuggingFace
 	     	<p>Inside ailab type: 
            <br>pip install huggingface_hub
            <br>huggingface-cli login
            <br>you will be asked for the token, copy it there (it doesn't show it, so make sure you type it and once)
            <br>you are asked "Add token as git credential? (Y/n)" sey Y
            <br>if you see "Login successful" than it passed - and yes, it looks like that...</p>
            <p><img src="images/HuggingFaceCli.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
 		    </li>

            <!--- L2 -->
   			<li>Download Llama
 	     	<p>Copy this code to a file:
            <br><a href="texts/download_llama.py" download>Download "download_llama.py" file</a>
            <br>save the file as download_llama.py
            <br>copy the file to ailab in Linux (like with did above),
            <br>Inside ailab type: 
            <br>python download_llama.py</p>
 		    </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- GNU Compiler (GCC) -->
    <p class="TitleList1Blue">GNU Compiler (GCC)</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- HuggingFace Token -->
		<p class="TitleList2Blue">GCC</p>
     	<ol>
            <!--- L2 -->
   			<li>Install GCC
 	     	<p>Inside Ubuntu Terminal: 
            <br>sudo apt update && sudo apt install build-essential -y
            <br>enter your password - we defined "dev" above (password it doesn't show when you type)</p>
            </li>

            <!--- L2 -->
   			<li>Verify GCC
 	     	<p>Inside Ubuntu Terminal: 
            <br>gcc --version</p>
            <p><img src="images/GccVer.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
            </li>
		</ol>
		</li>
    </ol>

 <!----------------------------------------------------------------------------------------------->

    <!--- Verify Installation - run Llama -->
    <p class="TitleList1Blue">Verify Installation - run Llama</p>
    <ol>
        <!--- L1 -->
        <li>
		<!--- Llama test -->
		<p class="TitleList2Blue">Llama test</p>
        <p>Verify that all the packages above are working and that you can get a simple Llama run.</p>
     	<ol>
            <!--- L2 -->
   			<li>Run Llama test
 	     	<p>Copy this code to a file:
            <br><a href="texts/test_llama.py" download>Download "test_llama.py" file</a>
            <br>save the file as test_llama.py
            <br>copy the file to ailab in Linux (like with did above),
            <br>Inside ailab type: 
            <br>python test_llama.py</p>
            </li>

            <!--- L2 -->
            <li>You might get errors
            <p>Errors on packages missing and so, it depends on the packages installed above etc.. 
            <br>and I got an error, 
            <br>Installed: pip install hf_transfer
            <br>got more errors,
            <p><img src="images/LlamaTestErrors.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
            <p>installed: pip install cut_cross_entropy msgspec protobuf sentencepiece torchao tyro datasets==3.4.1 trl==0.24.0</p>
            </li>

            <!--- L2 -->
            <li>Pass
            <p>It passes and generates a text:</p>
            <p><img src="images/LlamaTestPass.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
 		    </li>

            <!--- L2 -->
            <li>Watch
 	     	<p>You can watch live the memory progress on the GPU while you run the test, see it rising and getting back to 0
            <br>open another Terminal and type:
            <br>watch -n 1 nvidia-smi
            <p><img src="images/LlamaTestSmi.png" class="zoomable" ondblclick="this.classList.toggle('full')"></p>
 		    </li>
		</ol>
		</li>
    </ol>

</body>
</html>